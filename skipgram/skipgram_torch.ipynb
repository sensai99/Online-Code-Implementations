{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "94ac1502-3e4f-4092-aa64-5efbc40b947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "# Build Dataset\n",
    "# Negative Sampling\n",
    "# Define Model\n",
    "# Predictions & vector plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "34cbf4bc-8544-492c-a670-57e01cd9b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from preprocess_dataset import StanfordSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bb6ad2-6115-449a-b46c-596c7240ba45",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "d25cf859-eda8-45b5-9b80-111b5037ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path, window_size = 2, transform = None, target_transform = None):\n",
    "\n",
    "        # Get the raw dataset\n",
    "        dataset = StanfordSentiment(path)\n",
    "        self.corpus = dataset.get_dataset()\n",
    "\n",
    "        self.dataset = []\n",
    "        self.window_size = window_size\n",
    "        self.word2index = {}\n",
    "        self.vocab = []\n",
    "        self.word_freq = defaultdict(int)\n",
    "        self.construct_dataset()\n",
    "        \n",
    "    def construct_dataset(self):\n",
    "        for sentence in self.corpus:\n",
    "            for i, word in enumerate(sentence):\n",
    "                self.word_freq[word] += 1\n",
    "                if self.word2index.get(word) is None:\n",
    "                    self.word2index[word] = len(self.word2index)\n",
    "                \n",
    "                target_word_combinations = []\n",
    "                for k in range(i - self.window_size, i + self.window_size + 1):\n",
    "                    if k == i or k < 0 or k >= len(sentence):\n",
    "                        continue\n",
    "                    target_word_combinations.append([word] + [sentence[k]])\n",
    "                self.dataset.extend(target_word_combinations)\n",
    "                     \n",
    "        self.index2word = {i: w for w, i in self.word2index.items()}\n",
    "        self.vocab_size = len(self.index2word)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (torch.LongTensor([self.word2index[self.dataset[index][0]]]), torch.LongTensor([self.word2index[self.dataset[index][1]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "b74eda74-1d89-4590-958a-9753c629c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(path = './test.txt', window_size = 2)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size = 1, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f42d45-ff43-41cd-833b-ee589e7341ba",
   "metadata": {},
   "source": [
    "# Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "e349d3d1-01cd-4e19-86a5-ef1ff6c699e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sample_table_size = 1000000\n",
    "negative_samples_count = 1\n",
    "negative_sample_table = [0] * negative_sample_table_size\n",
    "\n",
    "def construct_negative_sample_table(dataset):\n",
    "    word_freq = [dataset.word_freq[dataset.index2word[i]] for i in range(dataset.vocab_size)]\n",
    "    word_freq = np.array(word_freq) ** 0.75\n",
    "    word_freq = word_freq / np.sum(word_freq)\n",
    "    word_freq = np.cumsum(word_freq) * negative_sample_table_size\n",
    "\n",
    "    j = 0\n",
    "    for i in range(0, negative_sample_table_size):\n",
    "        while i > word_freq[j]:\n",
    "            j += 1\n",
    "        negative_sample_table[i] = j\n",
    "\n",
    "def get_sample_word():\n",
    "     return negative_sample_table[random.randint(0, negative_sample_table_size - 1)]\n",
    "\n",
    "def get_negative_samples(target_word_index, context_word_index):\n",
    "    negative_samples = torch.LongTensor([])\n",
    "    \n",
    "    while negative_samples.shape[0] < negative_samples_count:\n",
    "        sample_word_index = get_sample_word()\n",
    "        while sample_word_index == target_word_index or sample_word_index == context_word_index:\n",
    "            sample_word_index = get_sample_word()\n",
    "        negative_samples = torch.cat((negative_samples, torch.LongTensor([sample_word_index])), dim = 0)\n",
    "    return negative_samples\n",
    "\n",
    "def generate_batch_negative_samples(target_word_indices, context_word_indices):\n",
    "    batches = target_word_indices.shape[0]\n",
    "\n",
    "    negative_samples = torch.LongTensor([])\n",
    "    for i in range(batches):\n",
    "        negative_sample = get_negative_samples(target_word_indices[i].data, context_word_indices[i].data)\n",
    "        negative_samples = torch.cat((negative_samples, negative_sample.unsqueeze(0)), dim = 0)\n",
    "\n",
    "    return negative_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "f631f4c1-d0a5-4851-9074-a0d47150a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_negative_sample_table(train_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9283778e-5209-4502-b44b-cb6073997c32",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "2226cb4f-cc93-46d1-9060-2e3d238fe207",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.hidden_layer = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.output_layer = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.logSigmoid = nn.LogSigmoid()\n",
    "\n",
    "        self.hidden_layer.weight.data.uniform_(-1.0, 1.0)\n",
    "        self.output_layer.weight.data.uniform_(-1.0, 1.0)\n",
    "        \n",
    "    def forward(self, target_word_indices, context_word_indices, negative_word_indices):\n",
    "        target_word_embeddings = self.hidden_layer(target_word_indices) # B x 1 x D\n",
    "        context_word_embeddings = self.output_layer(context_word_indices) # B x 1 x D\n",
    "        negative_word_embeddings = -self.output_layer(negative_word_indices) # B x K x D\n",
    "        \n",
    "\n",
    "        context_word_scores = (context_word_embeddings.bmm(target_word_embeddings.transpose(1, 2))).squeeze(2).squeeze(1) # 1D tensor with B elements\n",
    "        negative_word_scores = torch.sum((negative_word_embeddings.bmm(target_word_embeddings.transpose(1, 2))).squeeze(2), axis = 0) # 1D tensor with B elements\n",
    "        \n",
    "        pos_loss = self.logSigmoid(context_word_scores)\n",
    "        neg_loss = self.logSigmoid(negative_word_scores)\n",
    "        loss = pos_loss + neg_loss\n",
    "        return -torch.mean(loss)\n",
    "\n",
    "    def get_word_vector(self, index):\n",
    "        return self.hidden_layer(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "4cec7583-cf1c-497b-b05b-d92786ca1375",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(5, train_dataset.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53f110b-67e2-4823-8ec6-94b8fc280c4d",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "b5e829f1-9865-4cb5-a1e7-a3a1f0458e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "3ba7817e-2d34-4cea-ac3b-ec33559560d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "78d8b6ac-dabd-4ae4-abf8-e1fc4226bfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in tqdm(iterator, desc=\"Training\", leave=False):\n",
    "\n",
    "        target_word_indices = batch[0].to(device)\n",
    "        context_word_indices = batch[1].to(device)\n",
    "\n",
    "        negative_word_indices = generate_batch_negative_samples(target_word_indices, context_word_indices)\n",
    "        \n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        loss = model(target_word_indices, context_word_indices, negative_word_indices)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        loss = loss.item()\n",
    "\n",
    "        epoch_loss += loss\n",
    "\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "7fb18eb2-18f1-4369-a08a-f97c1e8f7087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d4a0f5fe7749c783da010d97a7ec30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 70.807\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02\n",
      "\tTrain Loss: 69.407\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03\n",
      "\tTrain Loss: 65.221\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04\n",
      "\tTrain Loss: 71.089\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05\n",
      "\tTrain Loss: 67.138\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06\n",
      "\tTrain Loss: 69.314\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07\n",
      "\tTrain Loss: 74.438\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08\n",
      "\tTrain Loss: 72.796\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09\n",
      "\tTrain Loss: 69.338\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n",
      "\tTrain Loss: 72.022\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in trange(EPOCHS):\n",
    "    epoch_loss = train(model, train_data_loader, optimizer, device)\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {epoch_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "d51bc9df-f192-4a0c-a594-23dd9e0df767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns top-10 similar words\n",
    "def word_similarity(target_word, vocab):\n",
    "    target_embedding = model.get_word_vector(torch.LongTensor([train_dataset.word2index[target_word]]))\n",
    "\n",
    "    similarities = []\n",
    "    for context_word in vocab:\n",
    "        if context_word == target_word: \n",
    "            continue\n",
    "        \n",
    "        context_embedding = model.get_word_vector(torch.LongTensor([train_dataset.word2index[context_word]]))\n",
    "        cosine_sim = F.cosine_similarity(target_embedding, context_embedding).data.tolist()[0]\n",
    "        similarities.append([context_word, cosine_sim])\n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "57563145-4434-4538-9192-09db46b19548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['car', 0.8025755882263184],\n",
       " ['apple', 0.6895413994789124],\n",
       " ['bacon', 0.6523947715759277],\n",
       " ['water', 0.6153475046157837],\n",
       " ['cherry', 0.59963059425354],\n",
       " ['berlin', 0.5709933042526245],\n",
       " ['mercedes', 0.5525842308998108],\n",
       " ['ford', 0.35669708251953125],\n",
       " ['usa', 0.3406466543674469],\n",
       " ['germany', 0.33181434869766235],\n",
       " ['mango', 0.2919435501098633],\n",
       " ['fruit', 0.19135074317455292],\n",
       " ['milk', 0.0682157501578331],\n",
       " ['boston', 0.026741784065961838],\n",
       " ['sugar', -0.12252910435199738],\n",
       " ['eat', -0.25700169801712036],\n",
       " ['cola', -0.3765031695365906],\n",
       " ['juice', -0.3999926447868347],\n",
       " ['cold', -0.6409810185432434]]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity('drink', train_dataset.word_freq.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c65b0f-2306-4e59-a170-65838f6174a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
