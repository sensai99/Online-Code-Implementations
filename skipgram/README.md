# Word2Vec
Word2Vec is a popular word embedding technique that learns vector representations of words from large text corpora.
This includes implementation of the Word2Vec Skip-gram model using negative sampling mechanism from scratch using Python and NumPy as well as the torch version. 

Steps:
* From Scratch Implementation:

   - Run `run.py`, which trains on the sample dataset `test.txt`
                
                python3 run.py
   - Generates `word_vectors.png` that shows 2D embeddings
* Pytorch: Run `skipgram_torch.ipynb`

*Note*: Highly recommend solving the theory part of [this](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/assignments/a2.pdf) assigment before implementation.