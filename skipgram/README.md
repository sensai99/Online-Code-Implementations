# Word2Vec
This is an implementation of the Word2Vec Skip-gram model using negative sampling mechanism from scratch using Python and NumPy. Word2Vec is a popular word embedding technique that learns vector representations of words from large text corpora.

Steps:
- Run run.py, which trains on the sample dataset `test.txt`
        
        python3 run.py
- Generates `word_vectors.png` that shows 2D embeddings

*Note*: Highly recommend solving the theory part of [this](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/assignments/a2.pdf) assigment before implementation.